{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arch import arch_model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, GRU, Conv1D, Flatten,Input, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_prices_train=np.load('real_train_data_array.npy')\n",
    "real_prices_test=np.load('real_test_data_array.npy')\n",
    "synthetic_prices_vae=np.load('synthetic_data_array_vae.npy')\n",
    "synthetic_prices_gan=np.load('synthetic_data_array_gan.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_returns_train=np.diff(real_prices_train, axis=1) / real_prices_train[:, :-1, :]\n",
    "real_returns_test=np.diff(real_prices_test, axis=1) / real_prices_test[:, :-1, :]\n",
    "synthetic_returns_vae=np.diff(synthetic_prices_vae, axis=1) / synthetic_prices_vae[:, :-1, :]\n",
    "synthetic_returns_gan=np.diff(synthetic_prices_gan, axis=1) / synthetic_prices_gan[:, :-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_volatility(returns, window_size):\n",
    "    \"\"\"\n",
    "    Calculate rolling volatility for a 3D array of returns.\n",
    "    \n",
    "    Parameters:\n",
    "    returns (numpy.ndarray): 3D array with dimensions (samples, periods, stocks) containing the returns.\n",
    "    window_size (int): The window size for calculating rolling volatility.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: 3D array with dimensions (samples, periods - window_size + 1, stocks) containing the rolling volatility.\n",
    "    \"\"\"\n",
    "    num_samples, num_periods, num_stocks = returns.shape\n",
    "    result_periods = num_periods - window_size + 1\n",
    "    rolling_volatility = np.zeros((num_samples, result_periods, num_stocks))\n",
    "    \n",
    "    for i in range(num_stocks):\n",
    "        for j in range(num_samples):\n",
    "            df = pd.Series(returns[j, :, i])\n",
    "            rolling_vol = df.rolling(window=window_size).std().values\n",
    "            # Fill NaNs with forward fill method\n",
    "            rolling_vol = pd.Series(rolling_vol).fillna(method='ffill').fillna(method='bfill').values\n",
    "            # Ensure correct slicing to avoid shape mismatch\n",
    "            rolling_volatility[j, :, i] = rolling_vol[window_size - 1:]\n",
    "    \n",
    "    return rolling_volatility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\capil\\AppData\\Local\\Temp\\ipykernel_7584\\3203889799.py:21: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  rolling_vol = pd.Series(rolling_vol).fillna(method='ffill').fillna(method='bfill').values\n"
     ]
    }
   ],
   "source": [
    "window_size=5\n",
    "real_vol_train = calculate_rolling_volatility(real_returns_train, window_size)\n",
    "real_vol_test = calculate_rolling_volatility(real_returns_test, window_size)\n",
    "synthetic_vol_vae = calculate_rolling_volatility(synthetic_returns_vae, window_size)\n",
    "synthetic_vol_gan = calculate_rolling_volatility(synthetic_returns_gan, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = real_vol_train.shape[1]\n",
    "n_seq = real_vol_test.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler_vae = MinMaxScaler()\n",
    "scaler_gan = MinMaxScaler()\n",
    "scaled_real_vol_train = scaler.fit_transform(real_vol_train.reshape(-1, 20)).reshape(-1, seq_len, 20)\n",
    "scaled_real_vol_test = scaler.transform(real_vol_test.reshape(-1, 20)).reshape(-1, seq_len, 20)\n",
    "scaled_synthetic_vol_vae = scaler_vae.fit_transform(synthetic_vol_vae.reshape(-1, 20)).reshape(-1, seq_len, 20)\n",
    "scaled_synthetic_vol_gan = scaler_gan.fit_transform(synthetic_vol_gan.reshape(-1, 20)).reshape(-1, seq_len, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_data = real_vol_train[:, :seq_len-1, :]\n",
    "real_train_label = real_vol_train[:, -1, :]\n",
    "\n",
    "scaled_real_train_data = scaled_real_vol_train[:, :seq_len-1, :]\n",
    "scaled_real_train_label = scaled_real_vol_train[:, -1, :]\n",
    "\n",
    "real_test_data = real_vol_test[:, :seq_len-1, :]\n",
    "real_test_label = real_vol_test[:, -1, :]\n",
    "garch_data=real_returns_test[:, -6:-1, :]\n",
    "scaled_real_test_data = scaled_real_vol_test[:, :seq_len-1, :]\n",
    "scaled_real_test_label = scaled_real_vol_test[:, -1, :]\n",
    "\n",
    "synthetic_train_vae = scaled_synthetic_vol_vae[:, :seq_len-1, :]\n",
    "synthetic_label_vae = scaled_synthetic_vol_vae[:, -1, :]\n",
    "synthetic_train_gan = scaled_synthetic_vol_gan[:, :seq_len-1, :]\n",
    "synthetic_label_gan = scaled_synthetic_vol_gan[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_train2_vae=np.concatenate((synthetic_train_vae, scaled_real_train_data), axis=0)\n",
    "synthetic_label2_vae=np.concatenate((synthetic_label_vae, scaled_real_train_label), axis=0)\n",
    "synthetic_train2_gan=np.concatenate((synthetic_train_gan, scaled_real_train_data), axis=0)\n",
    "synthetic_label2_gan=np.concatenate((synthetic_label_gan, scaled_real_train_label), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples\n",
    "num_samples = synthetic_train2_vae.shape[0]\n",
    "\n",
    "# Generate a permutation of indices\n",
    "indices = np.random.permutation(num_samples)\n",
    "\n",
    "# Shuffle the data and labels\n",
    "shuffled_synthetic_train2_vae = synthetic_train2_vae[indices]\n",
    "shuffled_synthetic_label2_vae = synthetic_label2_vae[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples\n",
    "num_samples = synthetic_train2_gan.shape[0]\n",
    "\n",
    "# Generate a permutation of indices\n",
    "indices = np.random.permutation(num_samples)\n",
    "\n",
    "# Shuffle the data and labels\n",
    "shuffled_synthetic_train2_gan = synthetic_train2_gan[indices]\n",
    "shuffled_synthetic_label2_gan = synthetic_label2_gan[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1D + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m 92/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0102\n",
      "Epoch 1: val_loss improved from inf to 0.00371, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0099 - val_loss: 0.0037\n",
      "Epoch 2/100\n",
      "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0045\n",
      "Epoch 2: val_loss improved from 0.00371 to 0.00319, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0045 - val_loss: 0.0032\n",
      "Epoch 3/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0038\n",
      "Epoch 3: val_loss improved from 0.00319 to 0.00283, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0038 - val_loss: 0.0028\n",
      "Epoch 4/100\n",
      "\u001b[1m 91/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034\n",
      "Epoch 4: val_loss improved from 0.00283 to 0.00246, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0025\n",
      "Epoch 5/100\n",
      "\u001b[1m 91/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0031\n",
      "Epoch 5: val_loss improved from 0.00246 to 0.00233, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 6/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0029\n",
      "Epoch 6: val_loss improved from 0.00233 to 0.00223, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 7/100\n",
      "\u001b[1m 93/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028\n",
      "Epoch 7: val_loss improved from 0.00223 to 0.00205, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0028 - val_loss: 0.0020\n",
      "Epoch 8/100\n",
      "\u001b[1m 93/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0026\n",
      "Epoch 8: val_loss improved from 0.00205 to 0.00194, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0026 - val_loss: 0.0019\n",
      "Epoch 9/100\n",
      "\u001b[1m 93/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026\n",
      "Epoch 9: val_loss improved from 0.00194 to 0.00189, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0026 - val_loss: 0.0019\n",
      "Epoch 10/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024\n",
      "Epoch 10: val_loss improved from 0.00189 to 0.00186, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 11/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023\n",
      "Epoch 11: val_loss improved from 0.00186 to 0.00181, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 12/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022\n",
      "Epoch 12: val_loss improved from 0.00181 to 0.00177, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 13/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023\n",
      "Epoch 13: val_loss improved from 0.00177 to 0.00174, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 14/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022\n",
      "Epoch 14: val_loss did not improve from 0.00174\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 15/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022\n",
      "Epoch 15: val_loss improved from 0.00174 to 0.00167, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 16/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021\n",
      "Epoch 16: val_loss improved from 0.00167 to 0.00165, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 17/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021\n",
      "Epoch 17: val_loss did not improve from 0.00165\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 18/100\n",
      "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021\n",
      "Epoch 18: val_loss improved from 0.00165 to 0.00161, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 19/100\n",
      "\u001b[1m 93/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020\n",
      "Epoch 19: val_loss did not improve from 0.00161\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 20/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021\n",
      "Epoch 20: val_loss did not improve from 0.00161\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 21/100\n",
      "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020\n",
      "Epoch 21: val_loss improved from 0.00161 to 0.00151, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 22/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021\n",
      "Epoch 22: val_loss did not improve from 0.00151\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 23/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021\n",
      "Epoch 23: val_loss did not improve from 0.00151\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 24/100\n",
      "\u001b[1m 97/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020\n",
      "Epoch 24: val_loss improved from 0.00151 to 0.00151, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 25/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020\n",
      "Epoch 25: val_loss did not improve from 0.00151\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 26/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020\n",
      "Epoch 26: val_loss did not improve from 0.00151\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 27/100\n",
      "\u001b[1m 97/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018\n",
      "Epoch 27: val_loss did not improve from 0.00151\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 28/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020\n",
      "Epoch 28: val_loss improved from 0.00151 to 0.00151, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 29/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020\n",
      "Epoch 29: val_loss did not improve from 0.00151\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 30/100\n",
      "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018\n",
      "Epoch 30: val_loss did not improve from 0.00151\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 31/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020\n",
      "Epoch 31: val_loss did not improve from 0.00151\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 32/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0018\n",
      "Epoch 32: val_loss improved from 0.00151 to 0.00151, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 33/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0019\n",
      "Epoch 33: val_loss improved from 0.00151 to 0.00150, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 34/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0020\n",
      "Epoch 34: val_loss improved from 0.00150 to 0.00148, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 35/100\n",
      "\u001b[1m 97/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0019\n",
      "Epoch 35: val_loss improved from 0.00148 to 0.00148, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 36/100\n",
      "\u001b[1m 97/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018\n",
      "Epoch 36: val_loss did not improve from 0.00148\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 37/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018\n",
      "Epoch 37: val_loss did not improve from 0.00148\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 38/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0019\n",
      "Epoch 38: val_loss did not improve from 0.00148\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 39/100\n",
      "\u001b[1m 97/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0018\n",
      "Epoch 39: val_loss did not improve from 0.00148\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 40/100\n",
      "\u001b[1m 97/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018\n",
      "Epoch 40: val_loss did not improve from 0.00148\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 41/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019\n",
      "Epoch 41: val_loss improved from 0.00148 to 0.00148, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 42/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0018\n",
      "Epoch 42: val_loss did not improve from 0.00148\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 43/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017\n",
      "Epoch 43: val_loss improved from 0.00148 to 0.00147, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 44/100\n",
      "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0018\n",
      "Epoch 44: val_loss improved from 0.00147 to 0.00142, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 45/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018\n",
      "Epoch 45: val_loss did not improve from 0.00142\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 46/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018\n",
      "Epoch 46: val_loss did not improve from 0.00142\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 47/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018\n",
      "Epoch 47: val_loss did not improve from 0.00142\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 48/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0018\n",
      "Epoch 48: val_loss did not improve from 0.00142\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 49/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0017\n",
      "Epoch 49: val_loss did not improve from 0.00142\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 50/100\n",
      "\u001b[1m 93/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0018\n",
      "Epoch 50: val_loss did not improve from 0.00142\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 51/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0017\n",
      "Epoch 51: val_loss did not improve from 0.00142\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 52/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017\n",
      "Epoch 52: val_loss did not improve from 0.00142\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 53/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0017\n",
      "Epoch 53: val_loss did not improve from 0.00142\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 54/100\n",
      "\u001b[1m 92/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017\n",
      "Epoch 54: val_loss did not improve from 0.00142\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 54: early stopping\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0716\n",
      "Test loss: 0.02567603811621666\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(18, 20)))\n",
    "model.add(LSTM(64, return_sequences=False))  # Return sequences should be False for the final prediction step\n",
    "model.add(Dense(20))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best.weights.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=False  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(scaled_real_train_data, scaled_real_train_label, epochs=100, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[checkpoint_callback,early_stopping_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "loss = model.evaluate(scaled_real_test_data, scaled_real_test_label)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0060\n",
      "Epoch 1: val_loss improved from inf to 0.00231, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0059 - val_loss: 0.0023\n",
      "Epoch 2/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 2: val_loss improved from 0.00231 to 0.00206, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 3/100\n",
      "\u001b[1m117/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019\n",
      "Epoch 3: val_loss improved from 0.00206 to 0.00177, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 4/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0016\n",
      "Epoch 4: val_loss improved from 0.00177 to 0.00156, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 5/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014\n",
      "Epoch 5: val_loss improved from 0.00156 to 0.00141, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 6/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 6: val_loss improved from 0.00141 to 0.00124, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 7/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 7: val_loss improved from 0.00124 to 0.00113, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 8/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.5041e-04\n",
      "Epoch 8: val_loss improved from 0.00113 to 0.00099, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 9.4996e-04 - val_loss: 9.9088e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.9326e-04\n",
      "Epoch 9: val_loss improved from 0.00099 to 0.00095, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 8.9279e-04 - val_loss: 9.5182e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4893e-04\n",
      "Epoch 10: val_loss improved from 0.00095 to 0.00092, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 8.4751e-04 - val_loss: 9.1901e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2859e-04\n",
      "Epoch 11: val_loss improved from 0.00092 to 0.00086, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 8.2774e-04 - val_loss: 8.5618e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.5903e-04\n",
      "Epoch 12: val_loss did not improve from 0.00086\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 7.5912e-04 - val_loss: 8.7899e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.5463e-04\n",
      "Epoch 13: val_loss improved from 0.00086 to 0.00084, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 7.5423e-04 - val_loss: 8.4499e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.0900e-04\n",
      "Epoch 14: val_loss improved from 0.00084 to 0.00083, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 7.0913e-04 - val_loss: 8.2662e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7860e-04\n",
      "Epoch 15: val_loss improved from 0.00083 to 0.00080, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.7934e-04 - val_loss: 8.0184e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8813e-04\n",
      "Epoch 16: val_loss improved from 0.00080 to 0.00077, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.8812e-04 - val_loss: 7.6761e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m115/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7662e-04\n",
      "Epoch 17: val_loss did not improve from 0.00077\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.7722e-04 - val_loss: 7.8521e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8197e-04\n",
      "Epoch 18: val_loss improved from 0.00077 to 0.00075, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.8117e-04 - val_loss: 7.4722e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5099e-04\n",
      "Epoch 19: val_loss did not improve from 0.00075\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.5106e-04 - val_loss: 7.8646e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.4854e-04\n",
      "Epoch 20: val_loss did not improve from 0.00075\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.4863e-04 - val_loss: 7.5238e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.4818e-04\n",
      "Epoch 21: val_loss improved from 0.00075 to 0.00074, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.4810e-04 - val_loss: 7.3512e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.2243e-04\n",
      "Epoch 22: val_loss improved from 0.00074 to 0.00072, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.2291e-04 - val_loss: 7.2391e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.4019e-04\n",
      "Epoch 23: val_loss did not improve from 0.00072\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.4016e-04 - val_loss: 7.3579e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.1576e-04\n",
      "Epoch 24: val_loss improved from 0.00072 to 0.00071, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.1636e-04 - val_loss: 7.1036e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.2013e-04\n",
      "Epoch 25: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.2016e-04 - val_loss: 7.2332e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.1770e-04\n",
      "Epoch 26: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.1773e-04 - val_loss: 7.2441e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.0718e-04\n",
      "Epoch 27: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.0740e-04 - val_loss: 7.3624e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m117/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.0815e-04\n",
      "Epoch 28: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.0782e-04 - val_loss: 7.6008e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.9378e-04\n",
      "Epoch 29: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.9392e-04 - val_loss: 7.1296e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.0159e-04\n",
      "Epoch 30: val_loss improved from 0.00071 to 0.00071, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.0145e-04 - val_loss: 7.0547e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.9379e-04\n",
      "Epoch 31: val_loss improved from 0.00071 to 0.00070, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.9397e-04 - val_loss: 7.0309e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.9067e-04\n",
      "Epoch 32: val_loss did not improve from 0.00070\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.9080e-04 - val_loss: 7.1060e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.8958e-04\n",
      "Epoch 33: val_loss did not improve from 0.00070\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.9003e-04 - val_loss: 7.1850e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.9542e-04\n",
      "Epoch 34: val_loss did not improve from 0.00070\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.9536e-04 - val_loss: 7.1363e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.8187e-04\n",
      "Epoch 35: val_loss improved from 0.00070 to 0.00069, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.8199e-04 - val_loss: 6.9075e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.7516e-04\n",
      "Epoch 36: val_loss did not improve from 0.00069\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.7545e-04 - val_loss: 7.3318e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.8962e-04\n",
      "Epoch 37: val_loss did not improve from 0.00069\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.8953e-04 - val_loss: 7.1256e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.7058e-04\n",
      "Epoch 38: val_loss did not improve from 0.00069\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.7117e-04 - val_loss: 6.9092e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.7198e-04\n",
      "Epoch 39: val_loss did not improve from 0.00069\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.7201e-04 - val_loss: 7.2405e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.7910e-04\n",
      "Epoch 40: val_loss improved from 0.00069 to 0.00068, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.7884e-04 - val_loss: 6.8327e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.7288e-04\n",
      "Epoch 41: val_loss did not improve from 0.00068\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.7284e-04 - val_loss: 6.8488e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.5198e-04\n",
      "Epoch 42: val_loss did not improve from 0.00068\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.5211e-04 - val_loss: 6.8706e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.7402e-04\n",
      "Epoch 43: val_loss did not improve from 0.00068\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.7391e-04 - val_loss: 7.2319e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.6605e-04\n",
      "Epoch 44: val_loss did not improve from 0.00068\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.6603e-04 - val_loss: 6.8783e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.4762e-04\n",
      "Epoch 45: val_loss did not improve from 0.00068\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.4776e-04 - val_loss: 6.9877e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.3420e-04\n",
      "Epoch 46: val_loss did not improve from 0.00068\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.3536e-04 - val_loss: 6.9858e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.4061e-04\n",
      "Epoch 47: val_loss did not improve from 0.00068\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.4074e-04 - val_loss: 7.0043e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.5531e-04\n",
      "Epoch 48: val_loss did not improve from 0.00068\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.5490e-04 - val_loss: 6.8901e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.4138e-04\n",
      "Epoch 49: val_loss did not improve from 0.00068\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.4155e-04 - val_loss: 6.8888e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.2543e-04\n",
      "Epoch 50: val_loss did not improve from 0.00068\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.2621e-04 - val_loss: 6.8819e-04\n",
      "Epoch 50: early stopping\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0966\n",
      "Test loss: 0.03984788805246353\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(18, 20)))\n",
    "model.add(LSTM(64, return_sequences=False))  # Return sequences should be False for the final prediction step\n",
    "model.add(Dense(20))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best.weights.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=False  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(synthetic_train_vae, synthetic_label_vae, epochs=100, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[checkpoint_callback,early_stopping_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "loss = model.evaluate(scaled_real_test_data, scaled_real_test_label)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real + Synthetic VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0064\n",
      "Epoch 1: val_loss improved from inf to 0.00350, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0064 - val_loss: 0.0035\n",
      "Epoch 2/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0031\n",
      "Epoch 2: val_loss improved from 0.00350 to 0.00291, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 3/100\n",
      "\u001b[1m217/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0023\n",
      "Epoch 3: val_loss improved from 0.00291 to 0.00259, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 4/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020\n",
      "Epoch 4: val_loss improved from 0.00259 to 0.00210, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 5/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0017\n",
      "Epoch 5: val_loss improved from 0.00210 to 0.00190, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 6/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0015\n",
      "Epoch 6: val_loss improved from 0.00190 to 0.00174, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 7/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0015\n",
      "Epoch 7: val_loss improved from 0.00174 to 0.00169, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 8/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014\n",
      "Epoch 8: val_loss improved from 0.00169 to 0.00160, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 9/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0014\n",
      "Epoch 9: val_loss did not improve from 0.00160\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 10/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014\n",
      "Epoch 10: val_loss improved from 0.00160 to 0.00152, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 11/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0013\n",
      "Epoch 11: val_loss did not improve from 0.00152\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 12/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0013\n",
      "Epoch 12: val_loss did not improve from 0.00152\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 13/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0013\n",
      "Epoch 13: val_loss did not improve from 0.00152\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 14/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 14: val_loss did not improve from 0.00152\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 15/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 15: val_loss improved from 0.00152 to 0.00149, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 16/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 16: val_loss improved from 0.00149 to 0.00146, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 17/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 17: val_loss did not improve from 0.00146\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 18/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 18: val_loss improved from 0.00146 to 0.00145, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 19/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 19: val_loss did not improve from 0.00145\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 20/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 20: val_loss did not improve from 0.00145\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 21/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 21: val_loss improved from 0.00145 to 0.00143, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 22/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 22: val_loss did not improve from 0.00143\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 23/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 23: val_loss did not improve from 0.00143\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 24/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 24: val_loss did not improve from 0.00143\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 25/100\n",
      "\u001b[1m217/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 25: val_loss improved from 0.00143 to 0.00143, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 26/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 26: val_loss improved from 0.00143 to 0.00142, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 27/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 27: val_loss did not improve from 0.00142\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 28/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0011\n",
      "Epoch 28: val_loss improved from 0.00142 to 0.00140, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 29/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0011\n",
      "Epoch 29: val_loss did not improve from 0.00140\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 30/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 30: val_loss did not improve from 0.00140\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 31/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 31: val_loss did not improve from 0.00140\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 32/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 32: val_loss did not improve from 0.00140\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 33/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 33: val_loss did not improve from 0.00140\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 34/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 34: val_loss did not improve from 0.00140\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 35/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 35: val_loss did not improve from 0.00140\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 36/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0010\n",
      "Epoch 36: val_loss did not improve from 0.00140\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 37/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0010\n",
      "Epoch 37: val_loss did not improve from 0.00140\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 38/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0010  \n",
      "Epoch 38: val_loss did not improve from 0.00140\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 38: early stopping\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0706\n",
      "Test loss: 0.02532365918159485\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(18, 20)))\n",
    "model.add(LSTM(64, return_sequences=False))  # Return sequences should be False for the final prediction step\n",
    "model.add(Dense(20))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best.weights.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=False  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(synthetic_train2_vae, synthetic_label2_vae, epochs=100, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[checkpoint_callback,early_stopping_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "loss = model.evaluate(scaled_real_test_data, scaled_real_test_label)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction_lstm_vae=model.predict(scaled_real_test_data)\n",
    "prediction_lstm_vae=scaler.inverse_transform(prediction_lstm_vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0029\n",
      "Epoch 1: val_loss improved from inf to 0.00149, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0015\n",
      "Epoch 2/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 2: val_loss improved from 0.00149 to 0.00093, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 9.2959e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.8730e-04\n",
      "Epoch 3: val_loss improved from 0.00093 to 0.00093, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 8.8826e-04 - val_loss: 9.2527e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.4788e-04\n",
      "Epoch 4: val_loss improved from 0.00093 to 0.00083, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 8.4806e-04 - val_loss: 8.3353e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.3927e-04\n",
      "Epoch 5: val_loss improved from 0.00083 to 0.00078, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 8.3860e-04 - val_loss: 7.7671e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0688e-04\n",
      "Epoch 6: val_loss improved from 0.00078 to 0.00075, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 8.0688e-04 - val_loss: 7.5446e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.8434e-04\n",
      "Epoch 7: val_loss improved from 0.00075 to 0.00074, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 7.8403e-04 - val_loss: 7.3902e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.3382e-04\n",
      "Epoch 8: val_loss did not improve from 0.00074\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 8.2902e-04 - val_loss: 7.5222e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.4451e-04\n",
      "Epoch 9: val_loss did not improve from 0.00074\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.4565e-04 - val_loss: 7.4223e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.0829e-04\n",
      "Epoch 10: val_loss improved from 0.00074 to 0.00070, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 7.0779e-04 - val_loss: 6.9788e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.1763e-04\n",
      "Epoch 11: val_loss did not improve from 0.00070\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.2075e-04 - val_loss: 7.3506e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.0708e-04\n",
      "Epoch 12: val_loss improved from 0.00070 to 0.00068, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 7.0417e-04 - val_loss: 6.8367e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3123e-04\n",
      "Epoch 13: val_loss improved from 0.00068 to 0.00067, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.3138e-04 - val_loss: 6.7317e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m115/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3143e-04\n",
      "Epoch 14: val_loss did not improve from 0.00067\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.3199e-04 - val_loss: 6.8221e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m116/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.1368e-04\n",
      "Epoch 15: val_loss did not improve from 0.00067\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.1598e-04 - val_loss: 6.9242e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.0106e-04\n",
      "Epoch 16: val_loss improved from 0.00067 to 0.00064, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.0111e-04 - val_loss: 6.3583e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7850e-04\n",
      "Epoch 17: val_loss improved from 0.00064 to 0.00062, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 6.7626e-04 - val_loss: 6.2178e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.3315e-04\n",
      "Epoch 18: val_loss did not improve from 0.00062\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 5.3724e-04 - val_loss: 6.4463e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.3429e-04\n",
      "Epoch 19: val_loss did not improve from 0.00062\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.3359e-04 - val_loss: 6.7922e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.6765e-04\n",
      "Epoch 20: val_loss did not improve from 0.00062\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.6792e-04 - val_loss: 8.0308e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.0141e-04\n",
      "Epoch 21: val_loss improved from 0.00062 to 0.00061, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.0117e-04 - val_loss: 6.1414e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.5169e-04\n",
      "Epoch 22: val_loss did not improve from 0.00061\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.5190e-04 - val_loss: 6.1828e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.5631e-04\n",
      "Epoch 23: val_loss improved from 0.00061 to 0.00060, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.5707e-04 - val_loss: 5.9569e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.3878e-04\n",
      "Epoch 24: val_loss did not improve from 0.00060\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.3904e-04 - val_loss: 6.7136e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.1436e-04\n",
      "Epoch 25: val_loss improved from 0.00060 to 0.00059, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.1615e-04 - val_loss: 5.8540e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.6700e-04\n",
      "Epoch 26: val_loss did not improve from 0.00059\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 5.6633e-04 - val_loss: 6.6438e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.0400e-04\n",
      "Epoch 27: val_loss did not improve from 0.00059\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.0515e-04 - val_loss: 7.9992e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.1830e-04\n",
      "Epoch 28: val_loss did not improve from 0.00059\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.1789e-04 - val_loss: 6.1568e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.1420e-04\n",
      "Epoch 29: val_loss improved from 0.00059 to 0.00057, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.1451e-04 - val_loss: 5.6890e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.0349e-04\n",
      "Epoch 30: val_loss did not improve from 0.00057\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.0347e-04 - val_loss: 5.8023e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.1771e-04\n",
      "Epoch 31: val_loss did not improve from 0.00057\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.1766e-04 - val_loss: 6.4269e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.3525e-04\n",
      "Epoch 32: val_loss improved from 0.00057 to 0.00056, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.3389e-04 - val_loss: 5.6238e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.8485e-04\n",
      "Epoch 33: val_loss improved from 0.00056 to 0.00056, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.8369e-04 - val_loss: 5.6141e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.9767e-04\n",
      "Epoch 34: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 4.9797e-04 - val_loss: 5.7885e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.1955e-04\n",
      "Epoch 35: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.1902e-04 - val_loss: 5.9000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.7441e-04\n",
      "Epoch 36: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.7498e-04 - val_loss: 5.8867e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.9657e-04\n",
      "Epoch 37: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.9587e-04 - val_loss: 5.9275e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.8975e-04\n",
      "Epoch 38: val_loss improved from 0.00056 to 0.00056, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.8945e-04 - val_loss: 5.6004e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.4759e-04\n",
      "Epoch 39: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.4848e-04 - val_loss: 5.8728e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.6856e-04\n",
      "Epoch 40: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.6852e-04 - val_loss: 5.7708e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.7107e-04\n",
      "Epoch 41: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.7193e-04 - val_loss: 6.3090e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.5018e-04\n",
      "Epoch 42: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.5032e-04 - val_loss: 5.8302e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.3042e-04\n",
      "Epoch 43: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.3070e-04 - val_loss: 6.3507e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.2151e-04\n",
      "Epoch 44: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.2248e-04 - val_loss: 5.7236e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.6661e-04\n",
      "Epoch 45: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.6562e-04 - val_loss: 6.1326e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.4086e-04\n",
      "Epoch 46: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.4083e-04 - val_loss: 5.6195e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.1417e-04\n",
      "Epoch 47: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 4.1435e-04 - val_loss: 5.6420e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.0765e-04\n",
      "Epoch 48: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.0793e-04 - val_loss: 5.6227e-04\n",
      "Epoch 48: early stopping\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0909\n",
      "Test loss: 0.03355742245912552\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(18, 20)))\n",
    "model.add(LSTM(64, return_sequences=False))  # Return sequences should be False for the final prediction step\n",
    "model.add(Dense(20))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best.weights.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=False  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(synthetic_train_gan, synthetic_label_gan, epochs=100, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[checkpoint_callback,early_stopping_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "loss = model.evaluate(scaled_real_test_data, scaled_real_test_label)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real + Synthetic GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0050\n",
      "Epoch 1: val_loss improved from inf to 0.00327, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0050 - val_loss: 0.0033\n",
      "Epoch 2/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0022\n",
      "Epoch 2: val_loss improved from 0.00327 to 0.00268, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 3/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018\n",
      "Epoch 3: val_loss improved from 0.00268 to 0.00241, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 4/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016\n",
      "Epoch 4: val_loss improved from 0.00241 to 0.00223, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 5/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0015\n",
      "Epoch 5: val_loss improved from 0.00223 to 0.00208, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 6/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0015\n",
      "Epoch 6: val_loss improved from 0.00208 to 0.00203, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 7/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014\n",
      "Epoch 7: val_loss improved from 0.00203 to 0.00186, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 8/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014\n",
      "Epoch 8: val_loss improved from 0.00186 to 0.00176, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 9/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0013\n",
      "Epoch 9: val_loss did not improve from 0.00176\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 10/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 10: val_loss improved from 0.00176 to 0.00171, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 11/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 11: val_loss did not improve from 0.00171\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 12/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0013\n",
      "Epoch 12: val_loss improved from 0.00171 to 0.00170, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 13/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0012\n",
      "Epoch 13: val_loss improved from 0.00170 to 0.00163, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 14/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0012\n",
      "Epoch 14: val_loss improved from 0.00163 to 0.00158, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 15/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 15: val_loss did not improve from 0.00158\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 16/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0011\n",
      "Epoch 16: val_loss did not improve from 0.00158\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 17/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 17: val_loss did not improve from 0.00158\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 18/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 18: val_loss did not improve from 0.00158\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 19/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 19: val_loss improved from 0.00158 to 0.00157, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 20/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0010\n",
      "Epoch 20: val_loss did not improve from 0.00157\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 21/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 21: val_loss improved from 0.00157 to 0.00152, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 22/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 22: val_loss did not improve from 0.00152\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 23/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0010\n",
      "Epoch 23: val_loss did not improve from 0.00152\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 24/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 24: val_loss improved from 0.00152 to 0.00149, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 25/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0010\n",
      "Epoch 25: val_loss did not improve from 0.00149\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 26/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.9987e-04\n",
      "Epoch 26: val_loss did not improve from 0.00149\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.9990e-04 - val_loss: 0.0015\n",
      "Epoch 27/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.6124e-04\n",
      "Epoch 27: val_loss did not improve from 0.00149\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.6178e-04 - val_loss: 0.0015\n",
      "Epoch 28/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.3206e-04\n",
      "Epoch 28: val_loss improved from 0.00149 to 0.00148, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.3295e-04 - val_loss: 0.0015\n",
      "Epoch 29/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.9486e-04\n",
      "Epoch 29: val_loss did not improve from 0.00148\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.9445e-04 - val_loss: 0.0015\n",
      "Epoch 30/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.6749e-04\n",
      "Epoch 30: val_loss did not improve from 0.00148\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.6753e-04 - val_loss: 0.0016\n",
      "Epoch 31/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0010\n",
      "Epoch 31: val_loss improved from 0.00148 to 0.00147, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 32/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.4412e-04\n",
      "Epoch 32: val_loss improved from 0.00147 to 0.00147, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 9.4399e-04 - val_loss: 0.0015\n",
      "Epoch 33/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.5344e-04\n",
      "Epoch 33: val_loss did not improve from 0.00147\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.5322e-04 - val_loss: 0.0015\n",
      "Epoch 34/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.3757e-04\n",
      "Epoch 34: val_loss did not improve from 0.00147\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.3765e-04 - val_loss: 0.0015\n",
      "Epoch 35/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.2779e-04\n",
      "Epoch 35: val_loss did not improve from 0.00147\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.2776e-04 - val_loss: 0.0015\n",
      "Epoch 36/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.0339e-04\n",
      "Epoch 36: val_loss did not improve from 0.00147\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.0362e-04 - val_loss: 0.0015\n",
      "Epoch 37/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.5404e-04\n",
      "Epoch 37: val_loss did not improve from 0.00147\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.5362e-04 - val_loss: 0.0015\n",
      "Epoch 38/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.1080e-04\n",
      "Epoch 38: val_loss did not improve from 0.00147\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.1078e-04 - val_loss: 0.0015\n",
      "Epoch 39/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.1069e-04\n",
      "Epoch 39: val_loss did not improve from 0.00147\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 9.1001e-04 - val_loss: 0.0015\n",
      "Epoch 40/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6753e-04\n",
      "Epoch 40: val_loss did not improve from 0.00147\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 8.6776e-04 - val_loss: 0.0015\n",
      "Epoch 41/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.8125e-04\n",
      "Epoch 41: val_loss did not improve from 0.00147\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 8.8122e-04 - val_loss: 0.0015\n",
      "Epoch 42/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6711e-04\n",
      "Epoch 42: val_loss did not improve from 0.00147\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 8.6722e-04 - val_loss: 0.0015\n",
      "Epoch 42: early stopping\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0699\n",
      "Test loss: 0.025124061852693558\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(18, 20)))\n",
    "model.add(LSTM(64, return_sequences=False))  # Return sequences should be False for the final prediction step\n",
    "model.add(Dense(20))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best.weights.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=False  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(synthetic_train2_gan, synthetic_label2_gan, epochs=100, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[checkpoint_callback,early_stopping_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "loss = model.evaluate(scaled_real_test_data, scaled_real_test_label)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction_lstm_gan=model.predict(real_test_data)\n",
    "prediction_lstm_gan=scaler.inverse_transform(prediction_lstm_gan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0142\n",
      "Epoch 1: val_loss improved from inf to 0.00417, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0141 - val_loss: 0.0042\n",
      "Epoch 2/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0050\n",
      "Epoch 2: val_loss improved from 0.00417 to 0.00317, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0050 - val_loss: 0.0032\n",
      "Epoch 3/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0040\n",
      "Epoch 3: val_loss improved from 0.00317 to 0.00285, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0040 - val_loss: 0.0028\n",
      "Epoch 4/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0034\n",
      "Epoch 4: val_loss improved from 0.00285 to 0.00257, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0034 - val_loss: 0.0026\n",
      "Epoch 5/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0033\n",
      "Epoch 5: val_loss improved from 0.00257 to 0.00229, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0032 - val_loss: 0.0023\n",
      "Epoch 6/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0027\n",
      "Epoch 6: val_loss improved from 0.00229 to 0.00210, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0027 - val_loss: 0.0021\n",
      "Epoch 7/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0027\n",
      "Epoch 7: val_loss improved from 0.00210 to 0.00198, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 8/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0026\n",
      "Epoch 8: val_loss improved from 0.00198 to 0.00188, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0026 - val_loss: 0.0019\n",
      "Epoch 9/100\n",
      "\u001b[1m 93/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0024\n",
      "Epoch 9: val_loss improved from 0.00188 to 0.00182, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 10/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0024\n",
      "Epoch 10: val_loss improved from 0.00182 to 0.00173, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 11/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0024\n",
      "Epoch 11: val_loss improved from 0.00173 to 0.00170, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 12/100\n",
      "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0022\n",
      "Epoch 12: val_loss improved from 0.00170 to 0.00164, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 13/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0023\n",
      "Epoch 13: val_loss improved from 0.00164 to 0.00160, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 14/100\n",
      "\u001b[1m 97/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0023\n",
      "Epoch 14: val_loss improved from 0.00160 to 0.00155, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 15/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0022\n",
      "Epoch 15: val_loss did not improve from 0.00155\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 16/100\n",
      "\u001b[1m 97/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0022\n",
      "Epoch 16: val_loss improved from 0.00155 to 0.00150, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 17/100\n",
      "\u001b[1m 93/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0022\n",
      "Epoch 17: val_loss did not improve from 0.00150\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 18/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 18: val_loss improved from 0.00150 to 0.00149, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 19/100\n",
      "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0022\n",
      "Epoch 19: val_loss improved from 0.00149 to 0.00146, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 20/100\n",
      "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 20: val_loss did not improve from 0.00146\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 21/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0021\n",
      "Epoch 21: val_loss improved from 0.00146 to 0.00140, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 22/100\n",
      "\u001b[1m 92/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0020\n",
      "Epoch 22: val_loss did not improve from 0.00140\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 23/100\n",
      "\u001b[1m 97/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0019\n",
      "Epoch 23: val_loss improved from 0.00140 to 0.00139, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 24/100\n",
      "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 24: val_loss did not improve from 0.00139\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 25/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0020\n",
      "Epoch 25: val_loss improved from 0.00139 to 0.00138, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 26/100\n",
      "\u001b[1m 92/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0020\n",
      "Epoch 26: val_loss improved from 0.00138 to 0.00138, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 27/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019\n",
      "Epoch 27: val_loss did not improve from 0.00138\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 28/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0020\n",
      "Epoch 28: val_loss did not improve from 0.00138\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 29/100\n",
      "\u001b[1m 93/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019\n",
      "Epoch 29: val_loss did not improve from 0.00138\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 30/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0021\n",
      "Epoch 30: val_loss improved from 0.00138 to 0.00137, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 31/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 31: val_loss improved from 0.00137 to 0.00135, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 32/100\n",
      "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 32: val_loss did not improve from 0.00135\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 33/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0020\n",
      "Epoch 33: val_loss did not improve from 0.00135\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 34/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0020\n",
      "Epoch 34: val_loss did not improve from 0.00135\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 35/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0022\n",
      "Epoch 35: val_loss did not improve from 0.00135\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 36/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019\n",
      "Epoch 36: val_loss did not improve from 0.00135\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 37/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0020\n",
      "Epoch 37: val_loss improved from 0.00135 to 0.00134, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 38/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 38: val_loss did not improve from 0.00134\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 39/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 39: val_loss did not improve from 0.00134\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 40/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 40: val_loss improved from 0.00134 to 0.00133, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 41/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0020\n",
      "Epoch 41: val_loss did not improve from 0.00133\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 42/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0019\n",
      "Epoch 42: val_loss improved from 0.00133 to 0.00133, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 43/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0020\n",
      "Epoch 43: val_loss did not improve from 0.00133\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 44/100\n",
      "\u001b[1m 97/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019\n",
      "Epoch 44: val_loss did not improve from 0.00133\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 45/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0021\n",
      "Epoch 45: val_loss improved from 0.00133 to 0.00131, saving model to best.weights.h5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 46/100\n",
      "\u001b[1m 97/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0018\n",
      "Epoch 46: val_loss did not improve from 0.00131\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 47/100\n",
      "\u001b[1m 93/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 47: val_loss did not improve from 0.00131\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 48/100\n",
      "\u001b[1m 92/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0020\n",
      "Epoch 48: val_loss did not improve from 0.00131\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 49/100\n",
      "\u001b[1m 95/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 49: val_loss did not improve from 0.00131\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 50/100\n",
      "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019\n",
      "Epoch 50: val_loss did not improve from 0.00131\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 51/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019\n",
      "Epoch 51: val_loss did not improve from 0.00131\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 52/100\n",
      "\u001b[1m 96/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0020\n",
      "Epoch 52: val_loss did not improve from 0.00131\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 53/100\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0020\n",
      "Epoch 53: val_loss did not improve from 0.00131\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 54/100\n",
      "\u001b[1m 91/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019\n",
      "Epoch 54: val_loss did not improve from 0.00131\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 55/100\n",
      "\u001b[1m 98/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0020\n",
      "Epoch 55: val_loss did not improve from 0.00131\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 55: early stopping\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0692\n",
      "Test loss: 0.02486472949385643\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([GRU(32, input_shape=(seq_len-1, n_seq)),\n",
    "                        Dense(20)])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best.weights.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=False  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(scaled_real_train_data, scaled_real_train_label, epochs=100, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[checkpoint_callback,early_stopping_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "loss = model.evaluate(scaled_real_test_data, scaled_real_test_label)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m117/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0065\n",
      "Epoch 1: val_loss improved from inf to 0.00268, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0063 - val_loss: 0.0027\n",
      "Epoch 2/100\n",
      "\u001b[1m117/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0023\n",
      "Epoch 2: val_loss improved from 0.00268 to 0.00198, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 3/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0018\n",
      "Epoch 3: val_loss improved from 0.00198 to 0.00162, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 4/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0014\n",
      "Epoch 4: val_loss improved from 0.00162 to 0.00139, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 5/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 5: val_loss improved from 0.00139 to 0.00120, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 6/100\n",
      "\u001b[1m115/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011\n",
      "Epoch 6: val_loss improved from 0.00120 to 0.00111, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 7/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.7156e-04\n",
      "Epoch 7: val_loss improved from 0.00111 to 0.00100, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 9.7138e-04 - val_loss: 9.9713e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.8403e-04\n",
      "Epoch 8: val_loss improved from 0.00100 to 0.00096, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 8.8412e-04 - val_loss: 9.6297e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.6723e-04\n",
      "Epoch 9: val_loss improved from 0.00096 to 0.00089, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 8.6543e-04 - val_loss: 8.8681e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m116/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1165e-04\n",
      "Epoch 10: val_loss improved from 0.00089 to 0.00087, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 8.1003e-04 - val_loss: 8.6772e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.5020e-04\n",
      "Epoch 11: val_loss improved from 0.00087 to 0.00082, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.5051e-04 - val_loss: 8.2344e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.5342e-04\n",
      "Epoch 12: val_loss improved from 0.00082 to 0.00080, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.5298e-04 - val_loss: 7.9554e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2014e-04\n",
      "Epoch 13: val_loss improved from 0.00080 to 0.00079, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.2021e-04 - val_loss: 7.8737e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m117/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2710e-04\n",
      "Epoch 14: val_loss improved from 0.00079 to 0.00078, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.2581e-04 - val_loss: 7.7605e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1891e-04\n",
      "Epoch 15: val_loss improved from 0.00078 to 0.00077, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.1767e-04 - val_loss: 7.6754e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8310e-04\n",
      "Epoch 16: val_loss did not improve from 0.00077\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.8343e-04 - val_loss: 7.7055e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6735e-04\n",
      "Epoch 17: val_loss improved from 0.00077 to 0.00075, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.6759e-04 - val_loss: 7.4745e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7859e-04\n",
      "Epoch 18: val_loss improved from 0.00075 to 0.00074, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.7859e-04 - val_loss: 7.3937e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m113/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7952e-04\n",
      "Epoch 19: val_loss did not improve from 0.00074\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.7901e-04 - val_loss: 7.4773e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6136e-04\n",
      "Epoch 20: val_loss did not improve from 0.00074\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.6165e-04 - val_loss: 7.4861e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m117/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5180e-04\n",
      "Epoch 21: val_loss did not improve from 0.00074\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.5297e-04 - val_loss: 7.4786e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4860e-04\n",
      "Epoch 22: val_loss did not improve from 0.00074\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.4916e-04 - val_loss: 7.4953e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m116/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6461e-04\n",
      "Epoch 23: val_loss improved from 0.00074 to 0.00073, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.6431e-04 - val_loss: 7.2732e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5220e-04\n",
      "Epoch 24: val_loss did not improve from 0.00073\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.5221e-04 - val_loss: 7.3926e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5302e-04\n",
      "Epoch 25: val_loss did not improve from 0.00073\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.5312e-04 - val_loss: 7.2762e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4895e-04\n",
      "Epoch 26: val_loss did not improve from 0.00073\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.4912e-04 - val_loss: 7.3545e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.6763e-04\n",
      "Epoch 27: val_loss did not improve from 0.00073\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.6689e-04 - val_loss: 7.3246e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5421e-04\n",
      "Epoch 28: val_loss improved from 0.00073 to 0.00072, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.5394e-04 - val_loss: 7.1582e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3972e-04\n",
      "Epoch 29: val_loss did not improve from 0.00072\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.3981e-04 - val_loss: 7.2795e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5020e-04\n",
      "Epoch 30: val_loss did not improve from 0.00072\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.5016e-04 - val_loss: 7.2292e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3369e-04\n",
      "Epoch 31: val_loss did not improve from 0.00072\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.3410e-04 - val_loss: 7.2286e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3587e-04\n",
      "Epoch 32: val_loss improved from 0.00072 to 0.00071, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.3589e-04 - val_loss: 7.1013e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m116/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4218e-04\n",
      "Epoch 33: val_loss improved from 0.00071 to 0.00071, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.4157e-04 - val_loss: 7.0592e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1762e-04\n",
      "Epoch 34: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.1843e-04 - val_loss: 7.2528e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4630e-04\n",
      "Epoch 35: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.4580e-04 - val_loss: 7.3010e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3280e-04\n",
      "Epoch 36: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.3280e-04 - val_loss: 7.1547e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2936e-04\n",
      "Epoch 37: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.2968e-04 - val_loss: 7.1993e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4367e-04\n",
      "Epoch 38: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.4304e-04 - val_loss: 7.0759e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3869e-04\n",
      "Epoch 39: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.3817e-04 - val_loss: 7.1065e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2441e-04\n",
      "Epoch 40: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.2487e-04 - val_loss: 7.0759e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m117/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.2377e-04\n",
      "Epoch 41: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.2402e-04 - val_loss: 7.0810e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3358e-04\n",
      "Epoch 42: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.3353e-04 - val_loss: 7.1675e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1781e-04\n",
      "Epoch 43: val_loss did not improve from 0.00071\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.1831e-04 - val_loss: 7.0809e-04\n",
      "Epoch 43: early stopping\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0866\n",
      "Test loss: 0.03365137428045273\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([GRU(32, input_shape=(seq_len-1, n_seq)),\n",
    "                        Dense(20)])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best.weights.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=False  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(synthetic_train_vae, synthetic_label_vae, epochs=100, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[checkpoint_callback,early_stopping_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "loss = model.evaluate(scaled_real_test_data, scaled_real_test_label)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real + Synthetic VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0120\n",
      "Epoch 1: val_loss improved from inf to 0.00323, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.0118 - val_loss: 0.0032\n",
      "Epoch 2/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0031\n",
      "Epoch 2: val_loss improved from 0.00323 to 0.00241, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 3/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0023\n",
      "Epoch 3: val_loss improved from 0.00241 to 0.00197, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 4/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0019\n",
      "Epoch 4: val_loss improved from 0.00197 to 0.00172, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 5/100\n",
      "\u001b[1m214/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0017\n",
      "Epoch 5: val_loss improved from 0.00172 to 0.00156, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 6/100\n",
      "\u001b[1m216/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0015\n",
      "Epoch 6: val_loss improved from 0.00156 to 0.00146, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 7/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0014\n",
      "Epoch 7: val_loss improved from 0.00146 to 0.00141, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 8/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0014\n",
      "Epoch 8: val_loss improved from 0.00141 to 0.00139, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 9/100\n",
      "\u001b[1m216/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0013\n",
      "Epoch 9: val_loss improved from 0.00139 to 0.00132, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 10/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 10: val_loss improved from 0.00132 to 0.00131, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 11/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 11: val_loss improved from 0.00131 to 0.00128, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 12/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 12: val_loss improved from 0.00128 to 0.00128, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 13/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 13: val_loss improved from 0.00128 to 0.00127, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 14/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 14: val_loss improved from 0.00127 to 0.00127, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 15/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 15: val_loss improved from 0.00127 to 0.00125, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 16/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 16: val_loss improved from 0.00125 to 0.00125, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 17/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 17: val_loss did not improve from 0.00125\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 18/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0013\n",
      "Epoch 18: val_loss improved from 0.00125 to 0.00125, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 19/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 19: val_loss did not improve from 0.00125\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 20/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 20: val_loss did not improve from 0.00125\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 21/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 21: val_loss improved from 0.00125 to 0.00124, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 22/100\n",
      "\u001b[1m217/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 22: val_loss improved from 0.00124 to 0.00124, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 23/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 23: val_loss did not improve from 0.00124\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 24/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 24: val_loss did not improve from 0.00124\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 25/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 25: val_loss improved from 0.00124 to 0.00124, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 26/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 26: val_loss did not improve from 0.00124\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 27/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 27: val_loss did not improve from 0.00124\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 28/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 28: val_loss did not improve from 0.00124\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 29/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 29: val_loss did not improve from 0.00124\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 30/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 30: val_loss did not improve from 0.00124\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 31/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 31: val_loss improved from 0.00124 to 0.00123, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 32/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 32: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 33/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 33: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 34/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 34: val_loss improved from 0.00123 to 0.00123, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 35/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 35: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 36/100\n",
      "\u001b[1m217/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 36: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 37/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 37: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 38/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 38: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 39/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 39: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 40/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 40: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 41/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 41: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 42/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 42: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 43/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0012\n",
      "Epoch 43: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 44/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 44: val_loss did not improve from 0.00123\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 44: early stopping\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0651\n",
      "Test loss: 0.02338969148695469\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([GRU(32, input_shape=(seq_len-1, n_seq)),\n",
    "                        Dense(20)])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best.weights.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=False  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(shuffled_synthetic_train2_vae, shuffled_synthetic_label2_vae, epochs=100, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[checkpoint_callback,early_stopping_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "loss = model.evaluate(scaled_real_test_data, scaled_real_test_label)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction_gru_vae=model.predict(scaled_real_test_data)\n",
    "prediction_gru_vae=scaler.inverse_transform(prediction_gru_vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0038\n",
      "Epoch 1: val_loss improved from inf to 0.00165, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0017\n",
      "Epoch 2/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0014\n",
      "Epoch 2: val_loss improved from 0.00165 to 0.00121, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 3/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011\n",
      "Epoch 3: val_loss improved from 0.00121 to 0.00092, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0011 - val_loss: 9.2401e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m116/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.1414e-04\n",
      "Epoch 4: val_loss improved from 0.00092 to 0.00086, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 9.1610e-04 - val_loss: 8.5984e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.4735e-04\n",
      "Epoch 5: val_loss improved from 0.00086 to 0.00080, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 9.4592e-04 - val_loss: 7.9744e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.4312e-04\n",
      "Epoch 6: val_loss improved from 0.00080 to 0.00077, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 8.4468e-04 - val_loss: 7.7306e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9231e-04\n",
      "Epoch 7: val_loss improved from 0.00077 to 0.00074, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.9423e-04 - val_loss: 7.3667e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9349e-04\n",
      "Epoch 8: val_loss improved from 0.00074 to 0.00070, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.9405e-04 - val_loss: 7.0430e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m115/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.5268e-04\n",
      "Epoch 9: val_loss improved from 0.00070 to 0.00070, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 8.4740e-04 - val_loss: 6.9740e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m114/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0341e-04\n",
      "Epoch 10: val_loss improved from 0.00070 to 0.00068, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.1030e-04 - val_loss: 6.7792e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1576e-04\n",
      "Epoch 11: val_loss did not improve from 0.00068\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.1600e-04 - val_loss: 6.8218e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.1939e-04\n",
      "Epoch 12: val_loss improved from 0.00068 to 0.00065, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 8.1700e-04 - val_loss: 6.5114e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m116/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7191e-04\n",
      "Epoch 13: val_loss did not improve from 0.00065\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 6.7622e-04 - val_loss: 7.7781e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5212e-04\n",
      "Epoch 14: val_loss improved from 0.00065 to 0.00065, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.5357e-04 - val_loss: 6.4698e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m116/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3812e-04\n",
      "Epoch 15: val_loss improved from 0.00065 to 0.00063, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.3598e-04 - val_loss: 6.3482e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4005e-04\n",
      "Epoch 16: val_loss improved from 0.00063 to 0.00063, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.4184e-04 - val_loss: 6.3128e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0629e-04\n",
      "Epoch 17: val_loss did not improve from 0.00063\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.0800e-04 - val_loss: 6.8084e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.9887e-04\n",
      "Epoch 18: val_loss improved from 0.00063 to 0.00062, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.9865e-04 - val_loss: 6.2257e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1198e-04\n",
      "Epoch 19: val_loss improved from 0.00062 to 0.00061, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.1088e-04 - val_loss: 6.1478e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2433e-04\n",
      "Epoch 20: val_loss did not improve from 0.00061\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.2450e-04 - val_loss: 6.2196e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1657e-04\n",
      "Epoch 21: val_loss did not improve from 0.00061\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.1750e-04 - val_loss: 6.3260e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m116/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6189e-04\n",
      "Epoch 22: val_loss improved from 0.00061 to 0.00061, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.6174e-04 - val_loss: 6.0979e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m115/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.9237e-04\n",
      "Epoch 23: val_loss did not improve from 0.00061\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.8794e-04 - val_loss: 6.6651e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2303e-04\n",
      "Epoch 24: val_loss did not improve from 0.00061\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.2351e-04 - val_loss: 6.2546e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9854e-04\n",
      "Epoch 25: val_loss did not improve from 0.00061\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.9994e-04 - val_loss: 6.0980e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.9313e-04\n",
      "Epoch 26: val_loss did not improve from 0.00061\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.9338e-04 - val_loss: 6.1734e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4272e-04\n",
      "Epoch 27: val_loss improved from 0.00061 to 0.00059, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.4185e-04 - val_loss: 5.8621e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.7159e-04\n",
      "Epoch 28: val_loss did not improve from 0.00059\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.7188e-04 - val_loss: 6.2526e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5242e-04\n",
      "Epoch 29: val_loss did not improve from 0.00059\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.5006e-04 - val_loss: 6.6641e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2351e-04\n",
      "Epoch 30: val_loss improved from 0.00059 to 0.00058, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.2177e-04 - val_loss: 5.8220e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.4691e-04\n",
      "Epoch 31: val_loss improved from 0.00058 to 0.00058, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 5.4852e-04 - val_loss: 5.8108e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m116/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.9032e-04\n",
      "Epoch 32: val_loss improved from 0.00058 to 0.00056, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 5.8950e-04 - val_loss: 5.5766e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.9814e-04\n",
      "Epoch 33: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 5.9759e-04 - val_loss: 5.6095e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m117/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1905e-04\n",
      "Epoch 34: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.1484e-04 - val_loss: 5.9538e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1009e-04\n",
      "Epoch 35: val_loss did not improve from 0.00056\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.0883e-04 - val_loss: 6.4139e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5660e-04\n",
      "Epoch 36: val_loss improved from 0.00056 to 0.00052, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 5.5638e-04 - val_loss: 5.1786e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7155e-04\n",
      "Epoch 37: val_loss did not improve from 0.00052\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.7287e-04 - val_loss: 5.2314e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.2576e-04\n",
      "Epoch 38: val_loss improved from 0.00052 to 0.00052, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.2563e-04 - val_loss: 5.1779e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.4009e-04\n",
      "Epoch 39: val_loss did not improve from 0.00052\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.3943e-04 - val_loss: 5.4064e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7794e-04\n",
      "Epoch 40: val_loss improved from 0.00052 to 0.00050, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.7814e-04 - val_loss: 5.0057e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.2040e-04\n",
      "Epoch 41: val_loss improved from 0.00050 to 0.00050, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 5.1883e-04 - val_loss: 4.9783e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.9914e-04\n",
      "Epoch 42: val_loss did not improve from 0.00050\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.9898e-04 - val_loss: 5.2915e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.0779e-04\n",
      "Epoch 43: val_loss did not improve from 0.00050\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.0700e-04 - val_loss: 5.0031e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.0056e-04\n",
      "Epoch 44: val_loss improved from 0.00050 to 0.00049, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.9961e-04 - val_loss: 4.8827e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5400e-04\n",
      "Epoch 45: val_loss did not improve from 0.00049\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.5568e-04 - val_loss: 5.6431e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4308e-04\n",
      "Epoch 46: val_loss improved from 0.00049 to 0.00049, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.4487e-04 - val_loss: 4.8805e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6864e-04\n",
      "Epoch 47: val_loss did not improve from 0.00049\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.6936e-04 - val_loss: 4.9622e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.3914e-04\n",
      "Epoch 48: val_loss did not improve from 0.00049\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.4061e-04 - val_loss: 6.0808e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7705e-04\n",
      "Epoch 49: val_loss did not improve from 0.00049\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.7690e-04 - val_loss: 5.3442e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7507e-04\n",
      "Epoch 50: val_loss did not improve from 0.00049\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.7569e-04 - val_loss: 5.0380e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0748e-04\n",
      "Epoch 51: val_loss did not improve from 0.00049\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.0924e-04 - val_loss: 5.3789e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.1054e-04\n",
      "Epoch 52: val_loss did not improve from 0.00049\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 5.0862e-04 - val_loss: 4.9090e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.3761e-04\n",
      "Epoch 53: val_loss improved from 0.00049 to 0.00048, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.3883e-04 - val_loss: 4.8353e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m117/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5942e-04\n",
      "Epoch 54: val_loss did not improve from 0.00048\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.5965e-04 - val_loss: 5.1041e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.9856e-04\n",
      "Epoch 55: val_loss did not improve from 0.00048\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.0037e-04 - val_loss: 4.8808e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m116/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0577e-04\n",
      "Epoch 56: val_loss did not improve from 0.00048\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.0989e-04 - val_loss: 5.1608e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5905e-04\n",
      "Epoch 57: val_loss did not improve from 0.00048\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.5902e-04 - val_loss: 4.9485e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.9097e-04\n",
      "Epoch 58: val_loss did not improve from 0.00048\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 3.9189e-04 - val_loss: 5.2136e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4388e-04\n",
      "Epoch 59: val_loss did not improve from 0.00048\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.4401e-04 - val_loss: 5.5066e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.3270e-04\n",
      "Epoch 60: val_loss improved from 0.00048 to 0.00047, saving model to best.weights.h5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.3288e-04 - val_loss: 4.7343e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.3245e-04\n",
      "Epoch 61: val_loss did not improve from 0.00047\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.3259e-04 - val_loss: 4.9808e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m115/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.6892e-04\n",
      "Epoch 62: val_loss did not improve from 0.00047\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 3.7525e-04 - val_loss: 4.9351e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m116/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4426e-04\n",
      "Epoch 63: val_loss did not improve from 0.00047\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.4447e-04 - val_loss: 5.1615e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.6062e-04\n",
      "Epoch 64: val_loss did not improve from 0.00047\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.6000e-04 - val_loss: 5.1584e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.0532e-04\n",
      "Epoch 65: val_loss did not improve from 0.00047\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.0700e-04 - val_loss: 4.9445e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.9667e-04\n",
      "Epoch 66: val_loss did not improve from 0.00047\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.9627e-04 - val_loss: 4.9629e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.3111e-04\n",
      "Epoch 67: val_loss did not improve from 0.00047\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.3146e-04 - val_loss: 4.9567e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6014e-04\n",
      "Epoch 68: val_loss did not improve from 0.00047\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.5967e-04 - val_loss: 4.8710e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m119/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.2817e-04\n",
      "Epoch 69: val_loss did not improve from 0.00047\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.2842e-04 - val_loss: 4.8418e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4496e-04\n",
      "Epoch 70: val_loss did not improve from 0.00047\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.4482e-04 - val_loss: 5.1527e-04\n",
      "Epoch 70: early stopping\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0834\n",
      "Test loss: 0.030349260196089745\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([GRU(32, input_shape=(seq_len-1, n_seq)),\n",
    "                        Dense(20)])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best.weights.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=False  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(synthetic_train_gan, synthetic_label_gan, epochs=100, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[checkpoint_callback,early_stopping_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "loss = model.evaluate(scaled_real_test_data, scaled_real_test_label)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real + Synthetic GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0070\n",
      "Epoch 1: val_loss improved from inf to 0.00290, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0069 - val_loss: 0.0029\n",
      "Epoch 2/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0024\n",
      "Epoch 2: val_loss improved from 0.00290 to 0.00209, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 3/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0019\n",
      "Epoch 3: val_loss improved from 0.00209 to 0.00178, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 4/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0017\n",
      "Epoch 4: val_loss improved from 0.00178 to 0.00159, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 5/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0015\n",
      "Epoch 5: val_loss improved from 0.00159 to 0.00146, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 6/100\n",
      "\u001b[1m217/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0014\n",
      "Epoch 6: val_loss did not improve from 0.00146\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 7/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014\n",
      "Epoch 7: val_loss improved from 0.00146 to 0.00134, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 8/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 8: val_loss improved from 0.00134 to 0.00132, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 9/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 9: val_loss improved from 0.00132 to 0.00128, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 10/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 10: val_loss did not improve from 0.00128\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 11/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 11: val_loss improved from 0.00128 to 0.00126, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 12/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 12: val_loss did not improve from 0.00126\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 13/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 13: val_loss improved from 0.00126 to 0.00125, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 14/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0013\n",
      "Epoch 14: val_loss improved from 0.00125 to 0.00125, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 15/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 15: val_loss improved from 0.00125 to 0.00124, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 16/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013\n",
      "Epoch 16: val_loss improved from 0.00124 to 0.00121, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 17/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 17: val_loss did not improve from 0.00121\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 18/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 18: val_loss did not improve from 0.00121\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 19/100\n",
      "\u001b[1m217/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 19: val_loss improved from 0.00121 to 0.00121, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 20/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011\n",
      "Epoch 20: val_loss did not improve from 0.00121\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 21/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 21: val_loss improved from 0.00121 to 0.00119, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 22/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 22: val_loss did not improve from 0.00119\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 23/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 23: val_loss improved from 0.00119 to 0.00118, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 24/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 24: val_loss improved from 0.00118 to 0.00117, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 25/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 25: val_loss did not improve from 0.00117\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 26/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 26: val_loss did not improve from 0.00117\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 27/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 27: val_loss did not improve from 0.00117\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 28/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 28: val_loss did not improve from 0.00117\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 29/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 29: val_loss did not improve from 0.00117\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 30/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011\n",
      "Epoch 30: val_loss did not improve from 0.00117\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 31/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 31: val_loss did not improve from 0.00117\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 32/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 32: val_loss improved from 0.00117 to 0.00117, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 33/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 33: val_loss improved from 0.00117 to 0.00116, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 34/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 34: val_loss improved from 0.00116 to 0.00115, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 35/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 35: val_loss did not improve from 0.00115\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 36/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012\n",
      "Epoch 36: val_loss did not improve from 0.00115\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 37/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 37: val_loss did not improve from 0.00115\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 38/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011\n",
      "Epoch 38: val_loss did not improve from 0.00115\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 39/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 39: val_loss improved from 0.00115 to 0.00115, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 40/100\n",
      "\u001b[1m224/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011\n",
      "Epoch 40: val_loss improved from 0.00115 to 0.00114, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 41/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 41: val_loss did not improve from 0.00114\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 42/100\n",
      "\u001b[1m223/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 42: val_loss did not improve from 0.00114\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 43/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 43: val_loss improved from 0.00114 to 0.00113, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 44/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 44: val_loss did not improve from 0.00113\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 45/100\n",
      "\u001b[1m219/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 45: val_loss improved from 0.00113 to 0.00112, saving model to best.weights.h5\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 46/100\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011\n",
      "Epoch 46: val_loss did not improve from 0.00112\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 47/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 47: val_loss did not improve from 0.00112\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 48/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 48: val_loss did not improve from 0.00112\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 49/100\n",
      "\u001b[1m221/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 49: val_loss did not improve from 0.00112\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 50/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011\n",
      "Epoch 50: val_loss did not improve from 0.00112\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 51/100\n",
      "\u001b[1m216/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 51: val_loss did not improve from 0.00112\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 52/100\n",
      "\u001b[1m220/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011\n",
      "Epoch 52: val_loss did not improve from 0.00112\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 53/100\n",
      "\u001b[1m222/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 53: val_loss did not improve from 0.00112\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 54/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 54: val_loss did not improve from 0.00112\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 55/100\n",
      "\u001b[1m218/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011\n",
      "Epoch 55: val_loss did not improve from 0.00112\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 55: early stopping\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0614\n",
      "Test loss: 0.022125618532299995\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([GRU(32, input_shape=(seq_len-1, n_seq)),\n",
    "                        Dense(20)])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best.weights.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=False  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(shuffled_synthetic_train2_gan, shuffled_synthetic_label2_gan, epochs=100, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[checkpoint_callback,early_stopping_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "loss = model.evaluate(scaled_real_test_data, scaled_real_test_label)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction_gru_gan=model.predict(scaled_real_test_data)\n",
    "prediction_gru_gan=scaler.inverse_transform(prediction_gru_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(978, 5, 20)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 9. The message is:\n",
      "Iteration limit reached\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\capil\\Desktop\\Volatility\\Trabajar\\extra_tfg\\myvenv\\lib\\site-packages\\arch\\univariate\\base.py:766: ConvergenceWarning: The optimizer returned code 8. The message is:\n",
      "Positive directional derivative for linesearch\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results_garch_1_1 = np.empty((garch_data.shape[0], garch_data.shape[2]))\n",
    "results_garch_2_1 = np.empty((garch_data.shape[0], garch_data.shape[2]))\n",
    "results_garch_1_2 = np.empty((garch_data.shape[0], garch_data.shape[2]))\n",
    "\n",
    "# Loop through the data and fit GARCH model\n",
    "for i in range(garch_data.shape[0]):\n",
    "    for j in range(garch_data.shape[2]):\n",
    "        model = arch_model(garch_data[i, :, j], vol='Garch', p=1, q=1, rescale=False)\n",
    "        model_fit = model.fit(disp='off')\n",
    "        forecasts = model_fit.forecast(horizon=1)\n",
    "        forecasted_variance = forecasts.variance.iloc[-1, 0]\n",
    "        forecasted_std_dev = np.sqrt(forecasted_variance)\n",
    "        results_garch_1_1[i, j] = forecasted_std_dev\n",
    "\n",
    "        model = arch_model(garch_data[i, :, j], vol='Garch', p=2, q=1, rescale=False)\n",
    "        model_fit = model.fit(disp='off')\n",
    "        forecasts = model_fit.forecast(horizon=1)\n",
    "        forecasted_variance = forecasts.variance.iloc[-1, 0]\n",
    "        forecasted_std_dev = np.sqrt(forecasted_variance)\n",
    "        results_garch_2_1[i, j] = forecasted_std_dev\n",
    "\n",
    "        model = arch_model(garch_data[i, :, j], vol='Garch', p=1, q=2, rescale=False)\n",
    "        model_fit = model.fit(disp='off')\n",
    "        forecasts = model_fit.forecast(horizon=1)\n",
    "        forecasted_variance = forecasts.variance.iloc[-1, 0]\n",
    "        forecasted_std_dev = np.sqrt(forecasted_variance)\n",
    "        results_garch_1_2[i, j] = forecasted_std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers=['AAPL', 'MSFT','JNJ', 'AMZN','XOM', 'JPM', 'LQD', 'SHY','IEF', 'TLT','^GSPC', \n",
    "         '^DJI', '^IXIC', 'GC=F', 'CL=F','IWM','FCEL', 'GBPUSD=X', 'JPY=X', 'EURUSD=X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df=pd.DataFrame(columns= [\"Mean Absolute Error Garch(1,1)\", \"Mean Absolute Error Garch(2,1)\", \"Mean Absolute Error Garch(1,2)\"])\n",
    "for k in range(20):\n",
    "    actual=real_test_label[:,k]\n",
    "\n",
    "    predicted_lstm_vae=prediction_lstm_vae[:,k]\n",
    "    predicted_lstm_gan=prediction_lstm_gan[:,k]\n",
    "    predicted_gru_vae=prediction_gru_vae[:,k]\n",
    "    predicted_gru_gan=prediction_gru_gan[:,k]\n",
    "\n",
    "    predicted_garch_1_1=results_garch_1_1[:,k]\n",
    "    predicted_garch_2_1=results_garch_2_1[:,k]\n",
    "    predicted_garch_1_2=results_garch_1_2[:,k]\n",
    "\n",
    "    mse_lstm_vae = mean_absolute_error(actual, predicted_lstm_vae)\n",
    "    mse_lstm_gan = mean_absolute_error(actual, predicted_lstm_gan)\n",
    "    mse_gru_vae = mean_absolute_error(actual, predicted_gru_vae)\n",
    "    mse_gru_gan = mean_absolute_error(actual, predicted_gru_gan)\n",
    "\n",
    "    mse_garch_1_1 = mean_absolute_error(actual, predicted_garch_1_1)\n",
    "    mse_garch_2_1 = mean_absolute_error(actual, predicted_garch_2_1)\n",
    "    mse_garch_1_2 = mean_absolute_error(actual, predicted_garch_1_2)\n",
    "\n",
    "    result_df.loc[k,'Mean Absolute Error LSTM VAE']=mse_lstm_vae\n",
    "    result_df.loc[k,'Mean Absolute Error LSTM GAN']=mse_lstm_gan\n",
    "    result_df.loc[k,'Mean Absolute Error GRU VAE']=mse_gru_vae\n",
    "    result_df.loc[k,'Mean Absolute Error GRU GAN']=mse_gru_gan\n",
    "\n",
    "    result_df.loc[k,'Mean Absolute Error Garch(1,1)']=mse_garch_1_1\n",
    "    result_df.loc[k,'Mean Absolute Error Garch(2,1)']=mse_garch_2_1\n",
    "    result_df.loc[k,'Mean Absolute Error Garch(1,2)']=mse_garch_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Absolute Error Garch(1,1)</th>\n",
       "      <th>Mean Absolute Error Garch(2,1)</th>\n",
       "      <th>Mean Absolute Error Garch(1,2)</th>\n",
       "      <th>Mean Absolute Error LSTM VAE</th>\n",
       "      <th>Mean Absolute Error LSTM GAN</th>\n",
       "      <th>Mean Absolute Error GRU VAE</th>\n",
       "      <th>Mean Absolute Error GRU GAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004022</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>0.016130</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.003417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003944</td>\n",
       "      <td>1.061441</td>\n",
       "      <td>0.004089</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.016146</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>0.003372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.002122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004752</td>\n",
       "      <td>0.005294</td>\n",
       "      <td>0.004906</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.018951</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.004508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004624</td>\n",
       "      <td>0.008738</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>0.004504</td>\n",
       "      <td>0.017803</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.004251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003765</td>\n",
       "      <td>0.009451</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.005268</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.004075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.015195</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.611592</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.010685</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.004037</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.009990</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.001931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.010685</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.002483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.002065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.009564</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.002921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.002053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008212</td>\n",
       "      <td>0.00911</td>\n",
       "      <td>0.008524</td>\n",
       "      <td>0.010624</td>\n",
       "      <td>0.028991</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.009833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.003853</td>\n",
       "      <td>0.003474</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>0.013944</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.002840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.012572</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.013387</td>\n",
       "      <td>0.035172</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>0.011455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.001079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.002091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.001154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Mean Absolute Error Garch(1,1) Mean Absolute Error Garch(2,1)  \\\n",
       "0                        0.004022                       0.004764   \n",
       "1                        0.003944                       1.061441   \n",
       "2                        0.002431                       0.003613   \n",
       "3                        0.004752                       0.005294   \n",
       "4                        0.004624                       0.008738   \n",
       "5                        0.003765                       0.009451   \n",
       "6                        0.001204                       0.001281   \n",
       "7                        0.015195                       0.004485   \n",
       "8                        0.001016                       0.010685   \n",
       "9                        0.002194                       0.002519   \n",
       "10                       0.002392                       0.002845   \n",
       "11                       0.002237                       0.002547   \n",
       "12                       0.003106                       0.003558   \n",
       "13                       0.002103                       0.003681   \n",
       "14                       0.008212                        0.00911   \n",
       "15                       0.003346                       0.003853   \n",
       "16                       0.012572                       0.014461   \n",
       "17                       0.001149                       0.001264   \n",
       "18                       0.001029                       0.001174   \n",
       "19                       0.000903                       0.001364   \n",
       "\n",
       "   Mean Absolute Error Garch(1,2)  Mean Absolute Error LSTM VAE  \\\n",
       "0                        0.004169                      0.003732   \n",
       "1                        0.004089                      0.004034   \n",
       "2                        0.002585                      0.002407   \n",
       "3                        0.004906                      0.004871   \n",
       "4                        0.004778                      0.004504   \n",
       "5                        0.003911                      0.005268   \n",
       "6                        0.001204                      0.001799   \n",
       "7                        0.611592                      0.000252   \n",
       "8                        0.000944                      0.000888   \n",
       "9                        0.002258                      0.002058   \n",
       "10                       0.002453                      0.002377   \n",
       "11                       0.002317                      0.002506   \n",
       "12                       0.009564                      0.003087   \n",
       "13                       0.002162                      0.002293   \n",
       "14                       0.008524                      0.010624   \n",
       "15                       0.003474                      0.003193   \n",
       "16                       0.013094                      0.013387   \n",
       "17                       0.001177                      0.001199   \n",
       "18                       0.001044                      0.002106   \n",
       "19                       0.000892                      0.001616   \n",
       "\n",
       "    Mean Absolute Error LSTM GAN  Mean Absolute Error GRU VAE  \\\n",
       "0                       0.016130                     0.003395   \n",
       "1                       0.016146                     0.003504   \n",
       "2                       0.009363                     0.002105   \n",
       "3                       0.018951                     0.004281   \n",
       "4                       0.017803                     0.004336   \n",
       "5                       0.014686                     0.003695   \n",
       "6                       0.005333                     0.001310   \n",
       "7                       0.000913                     0.000226   \n",
       "8                       0.004037                     0.000811   \n",
       "9                       0.009990                     0.001876   \n",
       "10                      0.010685                     0.002147   \n",
       "11                      0.009849                     0.002034   \n",
       "12                      0.012358                     0.002639   \n",
       "13                      0.007976                     0.001779   \n",
       "14                      0.028991                     0.010060   \n",
       "15                      0.013944                     0.002752   \n",
       "16                      0.035172                     0.012096   \n",
       "17                      0.004631                     0.001177   \n",
       "18                      0.004286                     0.001629   \n",
       "19                      0.003776                     0.001263   \n",
       "\n",
       "    Mean Absolute Error GRU GAN  \n",
       "0                      0.003417  \n",
       "1                      0.003372  \n",
       "2                      0.002122  \n",
       "3                      0.004508  \n",
       "4                      0.004251  \n",
       "5                      0.004075  \n",
       "6                      0.001226  \n",
       "7                      0.000218  \n",
       "8                      0.000889  \n",
       "9                      0.001931  \n",
       "10                     0.002483  \n",
       "11                     0.002065  \n",
       "12                     0.002921  \n",
       "13                     0.002053  \n",
       "14                     0.009833  \n",
       "15                     0.002840  \n",
       "16                     0.011455  \n",
       "17                     0.001079  \n",
       "18                     0.002091  \n",
       "19                     0.001154  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#min_values_per_row = result_df.min(axis=1)\n",
    "#result_df[result_df['Mean Absolute Error NN'] == min_values_per_row].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_values_per_row = result_df.min(axis=1)\n",
    "result_df[result_df['Mean Absolute Error GRU VAE'] == min_values_per_row].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_values_per_row = result_df.min(axis=1)\n",
    "result_df[result_df['Mean Absolute Error GRU GAN'] == min_values_per_row].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_values_per_row = result_df.min(axis=1)\n",
    "result_df[result_df[\"Mean Absolute Error Garch(1,1)\"] == min_values_per_row].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_values_per_row = result_df.min(axis=1)\n",
    "result_df[result_df[\"Mean Absolute Error Garch(2,1)\"] == min_values_per_row].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_values_per_row = result_df.min(axis=1)\n",
    "result_df[result_df[\"Mean Absolute Error Garch(1,2)\"] == min_values_per_row].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Absolute Error Garch(1,1)</th>\n",
       "      <th>Mean Absolute Error Garch(2,1)</th>\n",
       "      <th>Mean Absolute Error Garch(1,2)</th>\n",
       "      <th>Mean Absolute Error GRU VAE</th>\n",
       "      <th>Mean Absolute Error GRU GAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.004022</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.003417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.003944</td>\n",
       "      <td>1.061441</td>\n",
       "      <td>0.004089</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>0.003372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JNJ</th>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.002122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.004752</td>\n",
       "      <td>0.005294</td>\n",
       "      <td>0.004906</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.004508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>0.004624</td>\n",
       "      <td>0.008738</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.004251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JPM</th>\n",
       "      <td>0.003765</td>\n",
       "      <td>0.009451</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.004075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LQD</th>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHY</th>\n",
       "      <td>0.015195</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.611592</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IEF</th>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.010685</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TLT</th>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.001931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^GSPC</th>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.002483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^DJI</th>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.002065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^IXIC</th>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.009564</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.002921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GC=F</th>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.002053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL=F</th>\n",
       "      <td>0.008212</td>\n",
       "      <td>0.00911</td>\n",
       "      <td>0.008524</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.009833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IWM</th>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.003853</td>\n",
       "      <td>0.003474</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.002840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FCEL</th>\n",
       "      <td>0.012572</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>0.011455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBPUSD=X</th>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.001079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JPY=X</th>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.002091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EURUSD=X</th>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.001154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Mean Absolute Error Garch(1,1) Mean Absolute Error Garch(2,1)  \\\n",
       "AAPL                           0.004022                       0.004764   \n",
       "MSFT                           0.003944                       1.061441   \n",
       "JNJ                            0.002431                       0.003613   \n",
       "AMZN                           0.004752                       0.005294   \n",
       "XOM                            0.004624                       0.008738   \n",
       "JPM                            0.003765                       0.009451   \n",
       "LQD                            0.001204                       0.001281   \n",
       "SHY                            0.015195                       0.004485   \n",
       "IEF                            0.001016                       0.010685   \n",
       "TLT                            0.002194                       0.002519   \n",
       "^GSPC                          0.002392                       0.002845   \n",
       "^DJI                           0.002237                       0.002547   \n",
       "^IXIC                          0.003106                       0.003558   \n",
       "GC=F                           0.002103                       0.003681   \n",
       "CL=F                           0.008212                        0.00911   \n",
       "IWM                            0.003346                       0.003853   \n",
       "FCEL                           0.012572                       0.014461   \n",
       "GBPUSD=X                       0.001149                       0.001264   \n",
       "JPY=X                          0.001029                       0.001174   \n",
       "EURUSD=X                       0.000903                       0.001364   \n",
       "\n",
       "         Mean Absolute Error Garch(1,2)  Mean Absolute Error GRU VAE  \\\n",
       "AAPL                           0.004169                     0.003395   \n",
       "MSFT                           0.004089                     0.003504   \n",
       "JNJ                            0.002585                     0.002105   \n",
       "AMZN                           0.004906                     0.004281   \n",
       "XOM                            0.004778                     0.004336   \n",
       "JPM                            0.003911                     0.003695   \n",
       "LQD                            0.001204                     0.001310   \n",
       "SHY                            0.611592                     0.000226   \n",
       "IEF                            0.000944                     0.000811   \n",
       "TLT                            0.002258                     0.001876   \n",
       "^GSPC                          0.002453                     0.002147   \n",
       "^DJI                           0.002317                     0.002034   \n",
       "^IXIC                          0.009564                     0.002639   \n",
       "GC=F                           0.002162                     0.001779   \n",
       "CL=F                           0.008524                     0.010060   \n",
       "IWM                            0.003474                     0.002752   \n",
       "FCEL                           0.013094                     0.012096   \n",
       "GBPUSD=X                       0.001177                     0.001177   \n",
       "JPY=X                          0.001044                     0.001629   \n",
       "EURUSD=X                       0.000892                     0.001263   \n",
       "\n",
       "          Mean Absolute Error GRU GAN  \n",
       "AAPL                         0.003417  \n",
       "MSFT                         0.003372  \n",
       "JNJ                          0.002122  \n",
       "AMZN                         0.004508  \n",
       "XOM                          0.004251  \n",
       "JPM                          0.004075  \n",
       "LQD                          0.001226  \n",
       "SHY                          0.000218  \n",
       "IEF                          0.000889  \n",
       "TLT                          0.001931  \n",
       "^GSPC                        0.002483  \n",
       "^DJI                         0.002065  \n",
       "^IXIC                        0.002921  \n",
       "GC=F                         0.002053  \n",
       "CL=F                         0.009833  \n",
       "IWM                          0.002840  \n",
       "FCEL                         0.011455  \n",
       "GBPUSD=X                     0.001079  \n",
       "JPY=X                        0.002091  \n",
       "EURUSD=X                     0.001154  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.index=tickers\n",
    "result_df[['Mean Absolute Error Garch(1,1)', 'Mean Absolute Error Garch(2,1)',\n",
    "       'Mean Absolute Error Garch(1,2)','Mean Absolute Error GRU VAE',\n",
    "       'Mean Absolute Error GRU GAN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df=pd.DataFrame(columns= [\"Mean Absolute Error Garch(1,1)\", \"Mean Absolute Error Garch(2,1)\", \"Mean Absolute Error Garch(1,2)\"])\n",
    "for k in range(20):\n",
    "    actual=real_test_label[:,k]\n",
    "\n",
    "    predicted_lstm_vae=prediction_lstm_vae[:,k]\n",
    "    predicted_lstm_gan=prediction_lstm_gan[:,k]\n",
    "    predicted_gru_vae=prediction_gru_vae[:,k]\n",
    "    predicted_gru_gan=prediction_gru_gan[:,k]\n",
    "\n",
    "    predicted_garch_1_1=results_garch_1_1[:,k]\n",
    "    predicted_garch_2_1=results_garch_2_1[:,k]\n",
    "    predicted_garch_1_2=results_garch_1_2[:,k]\n",
    "\n",
    "    mse_lstm_vae = mean_squared_error(actual, predicted_lstm_vae)\n",
    "    mse_lstm_gan = mean_squared_error(actual, predicted_lstm_gan)\n",
    "    mse_gru_vae = mean_squared_error(actual, predicted_gru_vae)\n",
    "    mse_gru_gan = mean_squared_error(actual, predicted_gru_gan)\n",
    "\n",
    "    mse_garch_1_1 = mean_squared_error(actual, predicted_garch_1_1)\n",
    "    mse_garch_2_1 = mean_squared_error(actual, predicted_garch_2_1)\n",
    "    mse_garch_1_2 = mean_squared_error(actual, predicted_garch_1_2)\n",
    "\n",
    "    result_df.loc[k,'Mean Absolute Error LSTM VAE']=mse_lstm_vae\n",
    "    result_df.loc[k,'Mean Absolute Error LSTM GAN']=mse_lstm_gan\n",
    "    result_df.loc[k,'Mean Absolute Error GRU VAE']=mse_gru_vae\n",
    "    result_df.loc[k,'Mean Absolute Error GRU GAN']=mse_gru_gan\n",
    "\n",
    "    result_df.loc[k,'Mean Absolute Error Garch(1,1)']=mse_garch_1_1\n",
    "    result_df.loc[k,'Mean Absolute Error Garch(2,1)']=mse_garch_2_1\n",
    "    result_df.loc[k,'Mean Absolute Error Garch(1,2)']=mse_garch_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Absolute Error Garch(1,1)</th>\n",
       "      <th>Mean Absolute Error Garch(2,1)</th>\n",
       "      <th>Mean Absolute Error Garch(1,2)</th>\n",
       "      <th>Mean Absolute Error LSTM VAE</th>\n",
       "      <th>Mean Absolute Error LSTM GAN</th>\n",
       "      <th>Mean Absolute Error GRU VAE</th>\n",
       "      <th>Mean Absolute Error GRU GAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>4.834332e-05</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>3.514628e-05</td>\n",
       "      <td>2.953427e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>1092.437218</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>7.833026e-05</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>4.098685e-05</td>\n",
       "      <td>2.767448e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>1.643713e-05</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>1.250943e-05</td>\n",
       "      <td>1.205642e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>5.864503e-05</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>4.828410e-05</td>\n",
       "      <td>4.916298e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.011727</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>5.648127e-05</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>7.410638e-05</td>\n",
       "      <td>7.130809e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.027774</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>8.936054e-05</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>7.101673e-05</td>\n",
       "      <td>1.126925e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>4.612792e-05</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>1.176413e-05</td>\n",
       "      <td>5.859508e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.061844</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>359.467545</td>\n",
       "      <td>2.798203e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.344827e-07</td>\n",
       "      <td>1.186641e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.046038</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2.042625e-06</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>1.468252e-06</td>\n",
       "      <td>2.228913e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>1.230170e-05</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>8.049837e-06</td>\n",
       "      <td>9.205677e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>1.447749e-05</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>1.236175e-05</td>\n",
       "      <td>2.556211e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>1.812344e-05</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>1.426915e-05</td>\n",
       "      <td>1.254450e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.040211</td>\n",
       "      <td>2.342582e-05</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>1.620842e-05</td>\n",
       "      <td>4.185049e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.856845e-05</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>7.719949e-06</td>\n",
       "      <td>9.794685e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>5.294373e-03</td>\n",
       "      <td>0.009977</td>\n",
       "      <td>5.230777e-03</td>\n",
       "      <td>4.827405e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>4.232220e-05</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>1.845574e-05</td>\n",
       "      <td>1.790233e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>4.028490e-04</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>4.547565e-04</td>\n",
       "      <td>3.355668e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.950426e-06</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>3.912110e-06</td>\n",
       "      <td>2.781143e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.513545e-05</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.445706e-05</td>\n",
       "      <td>1.682763e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.00019</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.173502e-05</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>1.034670e-05</td>\n",
       "      <td>3.335635e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Mean Absolute Error Garch(1,1) Mean Absolute Error Garch(2,1)  \\\n",
       "0                        0.000044                       0.000064   \n",
       "1                        0.000043                    1092.437218   \n",
       "2                        0.000016                       0.000939   \n",
       "3                        0.000062                        0.00007   \n",
       "4                        0.000058                       0.011727   \n",
       "5                        0.000044                       0.027774   \n",
       "6                        0.000005                       0.000006   \n",
       "7                        0.061844                       0.004361   \n",
       "8                        0.000023                       0.046038   \n",
       "9                        0.000012                       0.000016   \n",
       "10                        0.00002                       0.000027   \n",
       "11                        0.00002                       0.000021   \n",
       "12                       0.000029                       0.000035   \n",
       "13                       0.000011                       0.001897   \n",
       "14                       0.002094                       0.002111   \n",
       "15                       0.000031                       0.000037   \n",
       "16                       0.000383                       0.000469   \n",
       "17                       0.000003                       0.000004   \n",
       "18                       0.000003                       0.000004   \n",
       "19                       0.000002                        0.00019   \n",
       "\n",
       "   Mean Absolute Error Garch(1,2)  Mean Absolute Error LSTM VAE  \\\n",
       "0                        0.000047                  4.834332e-05   \n",
       "1                        0.000045                  7.833026e-05   \n",
       "2                         0.00002                  1.643713e-05   \n",
       "3                        0.000064                  5.864503e-05   \n",
       "4                        0.000061                  5.648127e-05   \n",
       "5                        0.000048                  8.936054e-05   \n",
       "6                        0.000006                  4.612792e-05   \n",
       "7                      359.467545                  2.798203e-07   \n",
       "8                        0.000006                  2.042625e-06   \n",
       "9                        0.000013                  1.230170e-05   \n",
       "10                        0.00002                  1.447749e-05   \n",
       "11                       0.000021                  1.812344e-05   \n",
       "12                       0.040211                  2.342582e-05   \n",
       "13                       0.000011                  1.856845e-05   \n",
       "14                       0.002173                  5.294373e-03   \n",
       "15                       0.000032                  4.232220e-05   \n",
       "16                       0.000404                  4.028490e-04   \n",
       "17                       0.000003                  3.950426e-06   \n",
       "18                       0.000003                  1.513545e-05   \n",
       "19                       0.000002                  1.173502e-05   \n",
       "\n",
       "    Mean Absolute Error LSTM GAN  Mean Absolute Error GRU VAE  \\\n",
       "0                       0.000380                 3.514628e-05   \n",
       "1                       0.000381                 4.098685e-05   \n",
       "2                       0.000148                 1.250943e-05   \n",
       "3                       0.000477                 4.828410e-05   \n",
       "4                       0.000431                 7.410638e-05   \n",
       "5                       0.000374                 7.101673e-05   \n",
       "6                       0.000049                 1.176413e-05   \n",
       "7                       0.000002                 1.344827e-07   \n",
       "8                       0.000024                 1.468252e-06   \n",
       "9                       0.000136                 8.049837e-06   \n",
       "10                      0.000195                 1.236175e-05   \n",
       "11                      0.000186                 1.426915e-05   \n",
       "12                      0.000237                 1.620842e-05   \n",
       "13                      0.000088                 7.719949e-06   \n",
       "14                      0.009977                 5.230777e-03   \n",
       "15                      0.000279                 1.845574e-05   \n",
       "16                      0.001747                 4.547565e-04   \n",
       "17                      0.000030                 3.912110e-06   \n",
       "18                      0.000027                 1.445706e-05   \n",
       "19                      0.000018                 1.034670e-05   \n",
       "\n",
       "    Mean Absolute Error GRU GAN  \n",
       "0                  2.953427e-05  \n",
       "1                  2.767448e-05  \n",
       "2                  1.205642e-05  \n",
       "3                  4.916298e-05  \n",
       "4                  7.130809e-05  \n",
       "5                  1.126925e-04  \n",
       "6                  5.859508e-06  \n",
       "7                  1.186641e-07  \n",
       "8                  2.228913e-06  \n",
       "9                  9.205677e-06  \n",
       "10                 2.556211e-05  \n",
       "11                 1.254450e-05  \n",
       "12                 4.185049e-05  \n",
       "13                 9.794685e-06  \n",
       "14                 4.827405e-03  \n",
       "15                 1.790233e-05  \n",
       "16                 3.355668e-04  \n",
       "17                 2.781143e-06  \n",
       "18                 1.682763e-05  \n",
       "19                 3.335635e-06  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
